\newpage
# Resultados de entrenamiento {#results}

## Métrica: Valor-F

En análisis estadístico de clasificación binaria, el $\text{valor-F}$ es una medida de la **exactitud** de una prueba.
Considera tanto la **precisión** (del inglés _precision_) como la **exhaustividad** (del ingles _recall_). Su fórmula es la siguiente:

$$
F_\beta = (1 + \beta^2) \cdot \frac{\mathrm{precision} \cdot \mathrm{recall}}{(\beta^2 \cdot \mathrm{precision}) + \mathrm{recall}}
$$

En particular cuando $F_1$ ($\beta = 1$) es la media armónica de la precisión y la exhaustividad, donde un puntaje $F_1$ alcanza su mejor valor en 1 (precisión y _recall_ perfecta) y el peor en 0.

$$
F_1 = \left(\frac{2}{\mathrm{recall}^{-1} + \mathrm{precision}^{-1}}\right) = 2 \cdot \frac{\mathrm{precision} \cdot \mathrm{recall}}{\mathrm{precision} + \mathrm{recall}}
$$

Donde

 * $\text{precision}$ es el número de resultados positivos correctos dividido por el número de todos los resultados positivos devueltos por el clasificador.
 En particular para nuestro uso es el número de entidades nombradas que coinciden exactamente con el conjunto de evaluación.
 Por ejemplo, cuando se predice `[Persona Horacio] [Persona Gómez]`, pero lo correcto era `[Persona Horacio Gómez]`, la precisión es cero. La precisión es después promediada por cada una de la entidades nombradas.

$$
\text{precision}=\frac{|\{\text{resultados positivos}\}\cap\{\text{resultados correctos}\}|}{|\{\text{resultados positivos}\}|}
$$
 
 * $\text{recall}$ es el número de resultados positivos correctos dividido por el número todas las muestras que deberían haberse identificado como positivas.
En nuestro uso es el número de entidades del conjunto de evaluación que aparecen exactamente en la misma posición en las predicciones.

$$
\text{recall}=\frac{|\{\text{resultados positivos}\}\cap\{\text{resultados correctos}\}|}{|\{\text{resultados correctos}\}|}
$$

La métrica de $\text{valor-F}$ es ampliamente utilizada en la literatura del procesamiento de lenguaje natural _NLP_ [@pub_1084928040]; y en especial en el ámbito del reconocimiento de entidades nombradas _NER_.

Por esa razón hemos decidido utilizar la métrica $F_1$ para poder comparar nuestra **exactitud** con los resultados obtenidos por los entrenamientos de otras publicaciones y sistemas.

## Resultados obtenidos {#result-data}

La recomendación de _spaCy_ para entrenamientos de _NER_ es de al menos anotar 2000 textos, por lo que en este trabajo hemos etiquetado utilizando nuestra plataforma un número suficiente como para poder utilizar en el cálculo de resultados una **muestra aleatoria** de 4000 valores.

En los entrenamientos por refuerzo es necesario tomar la decisión de cuantos textos anotados serán utilizados para entrenar y cuantos serán utilizados para evaluar el entrenamiento.

Decidimos realizar diferentes pruebas y combinaciones de cantidad de documentos de entrenamiento y evaluación para analizar el $\text{valor-F}$ del modelo resultante:

* Entrenamiento:  0% (0 documentos) y evaluación: 0% (4000). Este escenario equivale a utilizar el modelo base de _spaCy_.
* Entrenamiento: 50% (2000 documentos) y evaluación: 50% (2000).
* Entrenamiento: 75% (3000 documentos) y evaluación: 25% (1000).
* Entrenamiento: 80% (3200 documentos) y evaluación: 20% (800).
* Entrenamiento: 85% (3400 documentos) y evaluación: 15% (600).
* Entrenamiento: 90% (3600 documentos) y evaluación: 10% (400).

En la figura \@ref(fig:fig-boxplot) se puede ver que nuestro sistema alcanza una exactitud en $\text{valor-F}$ de aproximadamente 80 puntos con relativamente pocos documentos.

```{r fig-boxplot, echo = FALSE, fig.pos="H", fig.align = 'center', out.width='80%', fig.asp=.75, fig.cap='Boxplot del Valor-F vs cantidad de documentos de entrenamiento'}
columns <- cols(
   Itn = col_double(),
   NER_Loss = col_double(),
   NER_P = col_double(),
   NER_R = col_double(),
   NER_F = col_double(),
   `Token_%` = col_double(),
   CPU_WPS = col_double(),
   GPU_WPS = col_double(),
   T_DOCS = col_double(),
   V_DOCS = col_double()
)
data <- read_delim(
  "data/results.csv",
  delim = "|",
  trim_ws = TRUE,
  col_types = columns
)
spacy_NER_F <- 66.238
p <- ggplot(data, aes(x=T_DOCS, group=T_DOCS, y=NER_F)) +
  geom_boxplot(varwidth = TRUE) +
  geom_hline(yintercept=spacy_NER_F) +
  annotate(geom="text", label="spaCy sin entrenar", x=2050, y=spacy_NER_F, vjust=-1) +
  xlab("Cantidad de documentos de entrenamiento") +
  ylab("Valor-F")
p
```

Sobre la cantidad de documentos necesarios para el entrenamiento vs evaluación podemos concluir que una relación 85% / 15% es la más acertada para un conjunto de $4000$ documentos.
