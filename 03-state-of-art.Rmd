\newpage
# Estado del arte {#state-of-art}

El análisis del estado del arte fue basado en la definición del problema.

## Stack de software

_Python_ es el lenguaje más utilizado para resolver problemas de Machine Learning, en especial _NLP_ [@github_machine_learning]

Spacy es el _framework_ mejor ranqueado para la tarea de _NLP_ [@github_machine_learning] y sabemos por la figura \@ref(fig:spacy-algos) que obtiene resultados a-la-par del estado del arte actual.

Además la implementación de _spaCy_ es robusta y orientada a la creación de apliciones para producción, a diferencia de muchas otras librerías de _NLP_ que sólo se utilizan con fines académicos.

## El pipeline

Todas las operaciones de analisis de lenguaje natural sobre textos no estructurados tienen como primer paso el de separar los mismos en tokens. Luego, el documento se procesa en varios pasos diferentes que consisten en el "pipeline de procesamiento". Usualmente los pasos consisten en un etiquetador, un analizador sintáctico y un reconocedor de entidades en el caso de _NER_.

Cada componente del pipeline mostrado en la figura \@ref(fig:spacy-pipeline) devuelve el un documento _Doc_ procesado, que luego se pasa al siguiente componente.

```{r spacy-pipeline, echo = FALSE, fig.pos="H", fig.align = 'center', fig.cap='Pipeline standard para los algoritmos de NER.'}
knitr::include_graphics('assets/spacy_pipeline.png')
```

En este capítulo estudiaremos la morfología de dicho pipeline.

## Algoritmo de tokenización

Para tokenizar un texto de manera correcta no basta con separar el mismo en espacios. Dependiendo del lenguaje que se esté estudiando, existen excepciones a esta regla y otros caracteres que representan separaciones entre tokens segun el contexto de los mismos.

En particular, _spaCy_ posee un algoritmo de tokenización inteligente que puede ser resumido de la siguiente manera:

1. Iterar sobre subcadenas separadas por espacios en blanco.
2. Comprobar si existe una regla definida explícitamente para esta subcadena. Si existe, usarla.
3. De lo contrario, intentar consumir un prefijo. Si consumimos un prefijo, regrese al punto #2, para que los casos especiales siempre tengan prioridad.
4. Si no se puede consumir un prefijo, intente consumir un sufijo y luego regrese al punto #2.
5. Si no se puede consumir un prefijo ni un sufijo, buscar un caso especial.
6. Buscar una coincidencia de token
7. Buscar "infijos" - cosas como guiones, etc. y dividir la subcadena en tokens en todos los infijos.
8. Una vez que no se pueda consumir más de la cadena, tratarla como un token único.

Ejemplo:

```{r spacy-tokenization, echo = FALSE, fig.pos="H", fig.align = 'center', fig.cap='Transiciones del modelo Stack-LSTM indicando la acción aplicada y el estado resultante.'}
knitr::include_graphics('assets/spacy_tokenization.png')
```

## Modelos basados en reglas

Antes de entrar en detalles de cómo trabaja el modelo estadístico de spacy y entender sus fortalezas es importante esbozar brevemente el grupo de algorítmos más "naive" posible. El de los modelos basados en reglas fijas.

En estos modelos se implementan reglas finitas o expresiones regulares para la detección de las entidades. Las principales limitaciones de este enfoque son:

* **Mucho trabajo manua**l: el sistema _Rule Based_ exige un profundo conocimiento del dominio, este analisis debe ser realizado por humanos expertos en el dominio.
* **Consumo de tiempo**: la generación de reglas para un sistema complejo es difícil y requiere mucho tiempo.
* **Menor capacidad de aprendizaje**: el sistema generará el resultado según las reglas, por lo que la capacidad de aprendizaje del sistema por sí mismo es baja.
* **Dominios complejos**: si el corpus demasiado complejo, la creación del sistema RB puede llevar mucho tiempo y análisis. La identificación de patrones complejos es una tarea desafiante en el enfoque RB.

## Modelos de "_deep-learning_"

Cuando se busca mejorar el aprendizaje automático, generalmente se piensa en la eficiencia y la precisión, pero la dimensión más importante es la generalidad. Este es el modelo estadístico que usa _spaCy_.

La mayoría de los problemas de `NLP` pueden reducirse a problemas de aprendizaje automático que toman uno o más textos como entrada. Si podemos transformar estos textos en vectores, podemos reutilizar soluciones de aprendizaje profundo (_deep-learning_) de propósito general.

Las representaciones de palabras embebidas (_embebed-words_), también conocidas como "vectores de palabras" (_word-vectors_), son una de las tecnologías de procesamiento de lenguaje natural más utilizadas en el estado del arte actual. El modelo de _deep learning_ utilizado por _spaCy_ puede ser descripto en cuatro pasos.

Los _word-embeddings_ permiten tratar a las palabras individuales como "unidades de significado", en lugar de identificaciones completamente distintas. A este proceso se le conoce como **_embed_**.

Sin embargo, la mayoría de los problemas de _NLP_ requieren la comprensión de tramos de texto más largos, no solo palabras individuales. Al juntar un conjunto de _word-embeddings_ en una secuencia de vectores, se usan RNN bidireccionales para codificar los vectores en una matriz de oración. Las filas de esta matriz pueden entenderse como "vectores-de-tokens": son sensibles al contexto del token dentro de la oración. Este paso se lo llama **_encode_**.

Finalmente, el mecanismo de **_attend_** le permite reducir la matriz de oración a un vector de oración, listo para la predicción (**_predict_**).

De esta manera quedan definidas las piezas que describen al modelo de _spaCy_:

```{r formula-shapes, echo = FALSE, fig.pos="H", fig.align = 'center', fig.cap='Estados posibles para las diferentes etapas de la CNN.'}
knitr::include_graphics('assets/deep-learning-formula-nlp_shapes.png')
```

Estos 4 procesos seran descriptos en detalle en las siguientes secciones.

### _Embed_
> Resuelve el problema: "todas las palabras se ven iguales para la computadora"

La idea de _word embeddings_ es la de "embeber" el conjunto de  tokens que componen términos con información adicional. El resultado de esta operación es una estructura abstracta que puede ser descripta como un "vector de significado".

```{r formula-embed, echo = FALSE, fig.pos="H", fig.align = 'center', fig.cap='Embed.'}
knitr::include_graphics('assets/deep-learning-formula-nlp_embed.svg')
```

Es importante destacar que en la etapa de _embed_, toda la información de significado es principalmente independiente del contexto en el cual esta siendo utilizada y por esta razón es facilmente obtenible del corpus no tagueado de datos (los algoritmos pueden darse cuenta de que palabras están relacionadas entre sí de manera eficiente y no supervisada).

Esto permite al modelo poder inferir significado a partir de la información no anotada dentro del problema en particular a resolver (el de _NER_).

Una tabla de _word-embeddings_ mapea vectores largos binarios y esparsos en vectores cortos densos y continuos, cargados de significado relevante. Existen varias estrategias para enriquecer los _tokens_ con información adicional. Las dos fuentes más grandes de información embebida son las de **información lingüística** y los **_word-vectors_**.

#### Información ligüística

El objetivo de ésta etapa es la de encontrar las características intrínsecas de cada palabra. Las principales características lingüísticas detectadas están resumidas en el siguiente ejemplo.

```{r formula-pos, echo = FALSE, fig.pos="H", fig.align = 'center', fig.cap='Características lingüísticas.'}
knitr::include_graphics('assets/pos.png')
```

* **Text**: la palabra original texto.
* **Lemma**: La forma básica de la palabra. `Es -> ser`. Esto permite a las siguientes etapas trabajar con una definicón canónica del token.
* **POS**: la etiqueta simple de parte-del-discurso (POS).
* **Tag**: la etiqueta detallada de parte del discurso (POS).
* **Dep**: dependencia sintáctica, es decir, la relación entre tokens.
* **Shape**: la forma de la palabra: mayúsculas, puntuación, dígitos. Muy útil para detectar patrones como números telefónicos, documentos de identidad, CBUs, etc.
* **is alpha**: ¿el token es una secuencia de caracteres alfabéticos?
* **is stop** : ¿es el token parte de una lista de "palabras de _stop_", es decir, las palabras más comunes del idioma?

##### _Word Vectors_

A diferencia de la información lingüística que es obtenida en el momento, existen otros tipos de _embeddings_ más poderosos que son los productos de pre-entrenamientos; como el caso de los _word-vectors_. Los mismos se generan mediante la concatenación de atributos léxicos como _NORM_, _PREFIX_, _SUFFIX_ y _SHAPE_. Luego se usa una capa oculta de una red neuronal convolucional (_CNN_) para permitir una combinación no lineal de la información en estos vectores concatenados.

Los _word-vectors_ son particularmente útiles para términos que no están bien representados en los datos de entrenamiento etiquetados. En nuestro uso de reconocimiento de entidades, no siempre habrá suficientes ocurrencias. Por ejemplo, en los datos de entrenamiento es posible que existan ocurrencias del término "Coca-Cola", pero ninguna del término "Manaos".
Es interesante pensar a las palabras como "vectores de significado". Dentro del espacio vectorial de significados el vector "perro" se encuentra cercano al de "cachorro", "beagle", "bulldog", "poodle". Esto permite al modelo poder inferir nuevas relaciones en base a una cantidad reducida de entradas.
Los _word-vectors_ ponen ese hecho a disposición del modelo de reconocimiento de entidades. Si bien no existen ejemplos de "Manaos" etiquetados como "Producto"; se verá que "Manaos" tiene un vector de palabras que generalmente corresponde a los términos de un producto, por lo que puede hacer la inferencia. Y si se quiere, se puede llegar incluso al detalle de que es una "Gaseosa".

Otra forma interesante de analizar y entender los _word-vectors_ en su contexto de espacio vectorial multidimensional de significados es a través del algebra de vectores, como se muestra en el trabajo "_Towards Understanding Linear Word Analogies_" [@ethayarajh-etal-2019-towards]:

```{r vec-parallelogram, echo = FALSE, fig.pos="H", fig.align = 'center', fig.cap='Parallelogram structure in the vector space (by definition).', out.width='60%'}
knitr::include_graphics('assets/parallelogram.png')
```

Es facil de entender viendo la figura \@ref(fig:vec-parallelogram) que al realizar algebra entre los diferentes vectores de significado se pueden inferir nuevos conceptos:

$$\vec{Rey} - \vec{Hombre} \approx \vec{Realeza}$$
$$\vec{Rey} - \vec{Hombre} + \vec{Mujer} \approx \vec{Reina}$$

### _Encode_ {#encode}
> Resuelve el problema: el contexto de los significados es relevante y esta siendo descartado,

El resultado de esta etapa es la de codificar vectores **independientes-de-contexto** en matrices de oración **sensibles-al-contexto**.

```{r formula-encode, echo = FALSE, fig.pos="H", fig.align = 'center', fig.cap='Encode.'}
knitr::include_graphics('assets/deep-learning-formula-nlp_encode.svg')
```

La tecnología utilizada para esta etapa es una Red Neuronal Convolucional en la que la oración es analizada como una _moving window_ de tres vectores en la que cada vector analiza su significado en relación con un vector previo y un vector posterior. Es decir, cada vector se estudia dentro del contexto de los dos vectores que lo rodean. Luego los vectores subsiguientes se evaluan de igual manera, por lo que se genera naturalmente un "efecto de decaimiento" en el que el contexto de los vectores más lejanos tiene una relevancia cada vez menor. 

### _Attend_
> Resuelve el problema: tenemos demasiada información para inferir un significado específico al problema a resolver.

En esta etapa toda la información generada en las etapas anteriores es analizada a través de un vector de entrada o tambien conocido como "vector de consulta" o "vector de contexto" representado en la figura \@ref(fig:formula-attend) como un vector mas oscuro.

```{r formula-attend, echo = FALSE, fig.pos="H", fig.align = 'center', fig.cap='Attend.'}
knitr::include_graphics('assets/deep-learning-formula-nlp_attend.svg')
```

Al reducir la matriz a un vector, necesariamente se está perdiendo información. Es por eso que el "vector de contexto" es crucial: Indica que información descartar, de modo que el "vector resumen" se adapte a la red que lo consume.

El análisis de estas estrategias de consulta escapa el alcance de este trabajo pero resulta un tema interesante de investigación en sí mismo. Por ejemplo, investigaciones recientes han demostrado que el mecanismo de atención es una técnica flexible y que se pueden usar nuevas variaciones para crear soluciones elegantes y poderosas. Por ejemplo, en el estudio de Ankur Parikh  et al [@parikh-etal-2016-decomposable] presentan un mecanismo de atención que toma dos matrices de oraciones y genera un solo vector.

### _Predict_ {#predict}
> Resuelve el problema: necesito un valor específico y no una representación genérica abstracta.

Finalmente en esta etapa tenemos un nuevo "vector de significado" que resulta de la consulta a la etapa anterior. Es necesario ahora traducir este vector a un _token_ efectivo. En el caso de _NER_, el _token_ que interesa obtener es el de la etiqueta de entidad.

```{r formula-predict, echo = FALSE, fig.pos="H", fig.align = 'center', fig.cap='Predict.'}
knitr::include_graphics('assets/deep-learning-formula-nlp_predict.svg')
```

## Uso del modelo estadístico {#use-of-cnn}

Experimentos en inglés, holandés, alemán y español muestran que se pueden obtener resultados a-la-par del estado del arte mediante el uso de un "autómata finito determinístico de pila" en conjunto con una red neuronal [@DBLP:journals/corr/LampleBSKD16].

Este autómata de pila es el nexo entre la Red Neuronal Convolucional (_CNN_) que contiene el modelo estadístico para predecir entidades en el texto completo. En el mismo, se envían cada uno de los estados porl los que se mueve para ir generando entidades con una herística del tipo _greedy_.

La figura \@ref(fig:lampe-1) muestra las posibles acciones de transición de este autómata.

* SHIFT: consume una token del input y al mueve al stack para generar una nueva entidad.
* REDUCE: mueve el stack actual al output tagueado como entity.
* OUT: consume una token del input y la mueve sl output directamente.

```{r lampe-1, echo = FALSE, fig.pos="H", fig.align = 'center', fig.cap='Transiciones del modelo Stack-LSTM indicando la acción aplicada y el estado resultante.'}
knitr::include_graphics('assets/lampe_1.png')
```

Para saber que acción tomar se consulta el modelo estadístico. En la figura \@ref(fig:lampe-2) se puede ver un ejemplo de como se recorre una oración bajo el stack propuesto:

```{r lampe-2, echo = FALSE, fig.pos="H", fig.align = 'center', fig.cap='Secuencia de tranciciones para el ejemplo "Mark Watney visited Mars" en el modelo de Stack-LSTM.'}
knitr::include_graphics('assets/lampe_2.png')
```

* Primero se empieza con un stack vacío.
* Se consume _"Mark"_ y la _CNN_ predice que es una posible Persona. Lo envia al stack.
* Se consume _"Watney"_ y la _CNN_ predice que es una posible continuación de Persona. Lo envia al stack.
* Se consume _"visited"_ y la _CNN_ predice que esto no forma parte de una entidad. Por lo tanto antes se _REDUCE_ la entidad _"Mark Watney"_ del stack actual.
* Análogamente se detecta la entidad _"Mars"_

La predicción que realiza la _CNN_ tiene un puntaje de 0 a 1; un porcentaje de certeza de que la token sea de un tipo específico. A veces las tokens pueden tener más de una etiqueta candidata a ser utilizada y éste es el método para desempatar este conflicto. Cuanto más cercano a $0.5$ es ese punaje mayor es la incerteza.


