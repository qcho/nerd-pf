<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 3 Estado del arte | NERd: anotador eficiente de modelos estadísticos para el reconocimiento de entidades nombradas</title>
  <meta name="description" content="Proyecto final de la carrera de ingeniería informática. ITBA." />
  <meta name="generator" content="bookdown 0.15.4 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 3 Estado del arte | NERd: anotador eficiente de modelos estadísticos para el reconocimiento de entidades nombradas" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Proyecto final de la carrera de ingeniería informática. ITBA." />
  <meta name="github-repo" content="qcho/nerd-pf" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 3 Estado del arte | NERd: anotador eficiente de modelos estadísticos para el reconocimiento de entidades nombradas" />
  
  <meta name="twitter:description" content="Proyecto final de la carrera de ingeniería informática. ITBA." />
  

<meta name="author" content="Horacio Miguel Gómez (L:50825)" />
<meta name="author" content="Juan Pablo Orsay (L:49373)" />
<meta name="author" content="Proyecto final de carrera" />


<meta name="date" content="2019-11-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="problem-definition.html"/>
<link rel="next" href="implementation.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="libs/d3-3.3.8/d3.min.js"></script>
<script src="libs/dagre-0.4.0/dagre-d3.min.js"></script>
<link href="libs/mermaid-0.3.0/dist/mermaid.css" rel="stylesheet" />
<script src="libs/mermaid-0.3.0/dist/mermaid.slim.min.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/chromatography-0.1/chromatography.js"></script>
<script src="libs/DiagrammeR-binding-1.0.0/DiagrammeR.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Named Entity Recognition Duh!</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introducción</a></li>
<li class="chapter" data-level="2" data-path="problem-definition.html"><a href="problem-definition.html"><i class="fa fa-check"></i><b>2</b> Definición del problema</a><ul>
<li class="chapter" data-level="2.1" data-path="problem-definition.html"><a href="problem-definition.html#conceptos-básicos"><i class="fa fa-check"></i><b>2.1</b> Conceptos básicos</a><ul>
<li class="chapter" data-level="2.1.1" data-path="problem-definition.html"><a href="problem-definition.html#entidad-nombrada"><i class="fa fa-check"></i><b>2.1.1</b> Entidad nombrada</a></li>
<li class="chapter" data-level="2.1.2" data-path="problem-definition.html"><a href="problem-definition.html#reconocimiento-de-entidades"><i class="fa fa-check"></i><b>2.1.2</b> Reconocimiento de entidades</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="problem-definition.html"><a href="problem-definition.html#los-datos-son-el-problema"><i class="fa fa-check"></i><b>2.2</b> Los datos son el problema</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="state-of-art.html"><a href="state-of-art.html"><i class="fa fa-check"></i><b>3</b> Estado del arte</a><ul>
<li class="chapter" data-level="3.1" data-path="state-of-art.html"><a href="state-of-art.html#stack-de-software"><i class="fa fa-check"></i><b>3.1</b> Stack de software</a></li>
<li class="chapter" data-level="3.2" data-path="state-of-art.html"><a href="state-of-art.html#el-pipeline"><i class="fa fa-check"></i><b>3.2</b> El pipeline</a></li>
<li class="chapter" data-level="3.3" data-path="state-of-art.html"><a href="state-of-art.html#algoritmo-de-tokenización"><i class="fa fa-check"></i><b>3.3</b> Algoritmo de tokenización</a></li>
<li class="chapter" data-level="3.4" data-path="state-of-art.html"><a href="state-of-art.html#modelos-basados-en-reglas"><i class="fa fa-check"></i><b>3.4</b> Modelos basados en reglas</a></li>
<li class="chapter" data-level="3.5" data-path="state-of-art.html"><a href="state-of-art.html#modelos-de-deep-learning"><i class="fa fa-check"></i><b>3.5</b> Modelos de “<em>deep-learning</em>”</a><ul>
<li class="chapter" data-level="3.5.1" data-path="state-of-art.html"><a href="state-of-art.html#embed"><i class="fa fa-check"></i><b>3.5.1</b> <em>Embed</em></a></li>
<li class="chapter" data-level="3.5.2" data-path="state-of-art.html"><a href="state-of-art.html#encode"><i class="fa fa-check"></i><b>3.5.2</b> <em>Encode</em></a></li>
<li class="chapter" data-level="3.5.3" data-path="state-of-art.html"><a href="state-of-art.html#attend"><i class="fa fa-check"></i><b>3.5.3</b> <em>Attend</em></a></li>
<li class="chapter" data-level="3.5.4" data-path="state-of-art.html"><a href="state-of-art.html#predict"><i class="fa fa-check"></i><b>3.5.4</b> <em>Predict</em></a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="state-of-art.html"><a href="state-of-art.html#use-of-cnn"><i class="fa fa-check"></i><b>3.6</b> Uso del modelo estadístico</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="implementation.html"><a href="implementation.html"><i class="fa fa-check"></i><b>4</b> NERd (Implementación)</a><ul>
<li class="chapter" data-level="4.1" data-path="implementation.html"><a href="implementation.html#vista-lógica"><i class="fa fa-check"></i><b>4.1</b> Vista lógica</a><ul>
<li class="chapter" data-level="4.1.1" data-path="implementation.html"><a href="implementation.html#web"><i class="fa fa-check"></i><b>4.1.1</b> Web</a></li>
<li class="chapter" data-level="4.1.2" data-path="implementation.html"><a href="implementation.html#api"><i class="fa fa-check"></i><b>4.1.2</b> <em>API</em></a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="implementation.html"><a href="implementation.html#vista-de-proceso"><i class="fa fa-check"></i><b>4.2</b> Vista de proceso</a><ul>
<li class="chapter" data-level="4.2.1" data-path="implementation.html"><a href="implementation.html#cliente-web"><i class="fa fa-check"></i><b>4.2.1</b> Cliente web</a></li>
<li class="chapter" data-level="4.2.2" data-path="implementation.html"><a href="implementation.html#api-1"><i class="fa fa-check"></i><b>4.2.2</b> <em>API</em></a></li>
<li class="chapter" data-level="4.2.3" data-path="implementation.html"><a href="implementation.html#worker"><i class="fa fa-check"></i><b>4.2.3</b> Worker</a></li>
<li class="chapter" data-level="4.2.4" data-path="implementation.html"><a href="implementation.html#task-scheduler"><i class="fa fa-check"></i><b>4.2.4</b> Task Scheduler</a></li>
<li class="chapter" data-level="4.2.5" data-path="implementation.html"><a href="implementation.html#base-de-datos"><i class="fa fa-check"></i><b>4.2.5</b> Base de datos</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="implementation.html"><a href="implementation.html#vista-de-desarrollo"><i class="fa fa-check"></i><b>4.3</b> Vista de desarrollo</a><ul>
<li class="chapter" data-level="4.3.1" data-path="implementation.html"><a href="implementation.html#cliente-web-1"><i class="fa fa-check"></i><b>4.3.1</b> Cliente web</a></li>
<li class="chapter" data-level="4.3.2" data-path="implementation.html"><a href="implementation.html#api-2"><i class="fa fa-check"></i><b>4.3.2</b> <em>API</em></a></li>
<li class="chapter" data-level="4.3.3" data-path="implementation.html"><a href="implementation.html#worker-1"><i class="fa fa-check"></i><b>4.3.3</b> Worker</a></li>
<li class="chapter" data-level="4.3.4" data-path="implementation.html"><a href="implementation.html#task-scheduler-1"><i class="fa fa-check"></i><b>4.3.4</b> <em>Task Scheduler</em></a></li>
<li class="chapter" data-level="4.3.5" data-path="implementation.html"><a href="implementation.html#MongoDBDev"><i class="fa fa-check"></i><b>4.3.5</b> Base de Datos</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="implementation.html"><a href="implementation.html#vista-física"><i class="fa fa-check"></i><b>4.4</b> Vista física</a><ul>
<li class="chapter" data-level="4.4.1" data-path="implementation.html"><a href="implementation.html#docker"><i class="fa fa-check"></i><b>4.4.1</b> <em>Docker</em></a></li>
<li class="chapter" data-level="4.4.2" data-path="implementation.html"><a href="implementation.html#deployment"><i class="fa fa-check"></i><b>4.4.2</b> Deployment</a></li>
<li class="chapter" data-level="4.4.3" data-path="implementation.html"><a href="implementation.html#servicios"><i class="fa fa-check"></i><b>4.4.3</b> Servicios</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="implementation.html"><a href="implementation.html#escenarios"><i class="fa fa-check"></i><b>4.5</b> Escenarios</a><ul>
<li class="chapter" data-level="4.5.1" data-path="implementation.html"><a href="implementation.html#inferencia-de-entidades-utilizando-el-api"><i class="fa fa-check"></i><b>4.5.1</b> Inferencia de entidades utilizando el API</a></li>
<li class="chapter" data-level="4.5.2" data-path="implementation.html"><a href="implementation.html#corrección-de-entidades-utilizando-el-entrenador"><i class="fa fa-check"></i><b>4.5.2</b> Corrección de entidades utilizando el entrenador</a></li>
<li class="chapter" data-level="4.5.3" data-path="implementation.html"><a href="implementation.html#habilitar-snapshots"><i class="fa fa-check"></i><b>4.5.3</b> Habilitar snapshots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="results.html"><a href="results.html"><i class="fa fa-check"></i><b>5</b> Resultados de entrenamiento</a><ul>
<li class="chapter" data-level="5.1" data-path="results.html"><a href="results.html#métrica-valor-f"><i class="fa fa-check"></i><b>5.1</b> Métrica: Valor-F</a></li>
<li class="chapter" data-level="5.2" data-path="results.html"><a href="results.html#result-data"><i class="fa fa-check"></i><b>5.2</b> Resultados obtenidos</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="discussion.html"><a href="discussion.html"><i class="fa fa-check"></i><b>6</b> Discusión</a><ul>
<li class="chapter" data-level="6.1" data-path="discussion.html"><a href="discussion.html#analisis-de-resultados"><i class="fa fa-check"></i><b>6.1</b> Analisis de resultados</a></li>
<li class="chapter" data-level="6.2" data-path="discussion.html"><a href="discussion.html#tipos-de-entidades"><i class="fa fa-check"></i><b>6.2</b> Tipos de entidades</a></li>
<li class="chapter" data-level="6.3" data-path="discussion.html"><a href="discussion.html#semilla-inicial"><i class="fa fa-check"></i><b>6.3</b> Semilla inicial</a></li>
<li class="chapter" data-level="6.4" data-path="discussion.html"><a href="discussion.html#entrenamiento-live-vs-offline"><i class="fa fa-check"></i><b>6.4</b> Entrenamiento <em>live</em> vs <em>offline</em></a></li>
<li class="chapter" data-level="6.5" data-path="discussion.html"><a href="discussion.html#linkeo-de-entidades-con-knowledge-base"><i class="fa fa-check"></i><b>6.5</b> <em>Linkeo</em> de entidades con <em>Knowledge Base</em></a></li>
<li class="chapter" data-level="6.6" data-path="discussion.html"><a href="discussion.html#utilidad-de-la-herramienta"><i class="fa fa-check"></i><b>6.6</b> Utilidad de la herramienta</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="conclusiones.html"><a href="conclusiones.html"><i class="fa fa-check"></i><b>7</b> Conclusiones</a></li>
<li class="chapter" data-level="" data-path="anexo-datos-de-resultados.html"><a href="anexo-datos-de-resultados.html"><i class="fa fa-check"></i>Anexo: Datos de resultados</a><ul>
<li class="chapter" data-level="7.1" data-path="anexo-datos-de-resultados.html"><a href="anexo-datos-de-resultados.html#t-0---v-100.-baseline-spacy"><i class="fa fa-check"></i><b>7.1</b> t: 0% - v: 100%. <em>Baseline spaCy</em></a></li>
<li class="chapter" data-level="7.2" data-path="anexo-datos-de-resultados.html"><a href="anexo-datos-de-resultados.html#t-50---v-50"><i class="fa fa-check"></i><b>7.2</b> t: 50% - v: 50%</a></li>
<li class="chapter" data-level="7.3" data-path="anexo-datos-de-resultados.html"><a href="anexo-datos-de-resultados.html#t-75---v-25"><i class="fa fa-check"></i><b>7.3</b> t: 75% - v: 25%</a></li>
<li class="chapter" data-level="7.4" data-path="anexo-datos-de-resultados.html"><a href="anexo-datos-de-resultados.html#t-80---v-20"><i class="fa fa-check"></i><b>7.4</b> t: 80% - v: 20%</a></li>
<li class="chapter" data-level="7.5" data-path="anexo-datos-de-resultados.html"><a href="anexo-datos-de-resultados.html#t-85---v-15"><i class="fa fa-check"></i><b>7.5</b> t: 85% - v: 15%</a></li>
<li class="chapter" data-level="7.6" data-path="anexo-datos-de-resultados.html"><a href="anexo-datos-de-resultados.html#t-90---v-10"><i class="fa fa-check"></i><b>7.6</b> t: 90% - v: 10%</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="glosario.html"><a href="glosario.html"><i class="fa fa-check"></i>Glosario</a></li>
<li class="chapter" data-level="" data-path="referencias.html"><a href="referencias.html"><i class="fa fa-check"></i>Referencias</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">NERd: anotador eficiente de modelos estadísticos para el reconocimiento de entidades nombradas</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="state-of-art" class="section level1">
<h1><span class="header-section-number">Capítulo 3</span> Estado del arte</h1>
<p>El análisis del estado del arte fue basado en la definición del problema.</p>
<div id="stack-de-software" class="section level2">
<h2><span class="header-section-number">3.1</span> Stack de software</h2>
<p><em>Python</em> es el lenguaje más utilizado para resolver problemas de Machine Learning, en especial <em>NLP</em> <span class="citation">(Elliott, <a href="#ref-github_machine_learning">2019</a>)</span></p>
<p>Spacy es el <em>framework</em> mejor ranqueado para la tarea de <em>NLP</em> <span class="citation">(Elliott, <a href="#ref-github_machine_learning">2019</a>)</span> y sabemos por la figura <a href="problem-definition.html#fig:spacy-algos">2.1</a> que obtiene resultados a-la-par del estado del arte actual.</p>
<p>Además la implementación de <em>spaCy</em> es robusta y orientada a la creación de apliciones para producción, a diferencia de muchas otras librerías de <em>NLP</em> que sólo se utilizan con fines académicos.</p>
</div>
<div id="el-pipeline" class="section level2">
<h2><span class="header-section-number">3.2</span> El pipeline</h2>
<p>Todas las operaciones de analisis de lenguaje natural sobre textos no estructurados tienen como primer paso el de separar los mismos en tokens. Luego, el documento se procesa en varios pasos diferentes que consisten en el “pipeline de procesamiento”. Usualmente los pasos consisten en un etiquetador, un analizador sintáctico y un reconocedor de entidades en el caso de <em>NER</em>.</p>
<p>Cada componente del pipeline mostrado en la figura <a href="state-of-art.html#fig:spacy-pipeline">3.1</a> devuelve el un documento <em>Doc</em> procesado, que luego se pasa al siguiente componente.</p>
<div class="figure" style="text-align: center"><span id="fig:spacy-pipeline"></span>
<img src="assets/spacy_pipeline.png" alt="Pipeline standard para los algoritmos de NER."  />
<p class="caption">
Figura 3.1: Pipeline standard para los algoritmos de NER.
</p>
</div>
<p>En este capítulo estudiaremos la morfología de dicho pipeline.</p>
</div>
<div id="algoritmo-de-tokenización" class="section level2">
<h2><span class="header-section-number">3.3</span> Algoritmo de tokenización</h2>
<p>Para tokenizar un texto de manera correcta no basta con separar el mismo en espacios. Dependiendo del lenguaje que se esté estudiando, existen excepciones a esta regla y otros caracteres que representan separaciones entre tokens segun el contexto de los mismos.</p>
<p>En particular, <em>spaCy</em> posee un algoritmo de tokenización inteligente que puede ser resumido de la siguiente manera:</p>
<ol style="list-style-type: decimal">
<li>Iterar sobre subcadenas separadas por espacios en blanco.</li>
<li>Comprobar si existe una regla definida explícitamente para esta subcadena. Si existe, usarla.</li>
<li>De lo contrario, intentar consumir un prefijo. Si consumimos un prefijo, regrese al punto #2, para que los casos especiales siempre tengan prioridad.</li>
<li>Si no se puede consumir un prefijo, intente consumir un sufijo y luego regrese al punto #2.</li>
<li>Si no se puede consumir un prefijo ni un sufijo, buscar un caso especial.</li>
<li>Buscar una coincidencia de token</li>
<li>Buscar “infijos” - cosas como guiones, etc. y dividir la subcadena en tokens en todos los infijos.</li>
<li>Una vez que no se pueda consumir más de la cadena, tratarla como un token único.</li>
</ol>
<p>Ejemplo:</p>
<div class="figure" style="text-align: center"><span id="fig:spacy-tokenization"></span>
<img src="assets/spacy_tokenization.png" alt="Transiciones del modelo Stack-LSTM indicando la acción aplicada y el estado resultante."  />
<p class="caption">
Figura 3.2: Transiciones del modelo Stack-LSTM indicando la acción aplicada y el estado resultante.
</p>
</div>
</div>
<div id="modelos-basados-en-reglas" class="section level2">
<h2><span class="header-section-number">3.4</span> Modelos basados en reglas</h2>
<p>Antes de entrar en detalles de cómo trabaja el modelo estadístico de spacy y entender sus fortalezas es importante esbozar brevemente el grupo de algorítmos más “naive” posible. El de los modelos basados en reglas fijas.</p>
<p>En estos modelos se implementan reglas finitas o expresiones regulares para la detección de las entidades. Las principales limitaciones de este enfoque son:</p>
<ul>
<li><strong>Mucho trabajo manua</strong>l: el sistema <em>Rule Based</em> exige un profundo conocimiento del dominio, este analisis debe ser realizado por humanos expertos en el dominio.</li>
<li><strong>Consumo de tiempo</strong>: la generación de reglas para un sistema complejo es difícil y requiere mucho tiempo.</li>
<li><strong>Menor capacidad de aprendizaje</strong>: el sistema generará el resultado según las reglas, por lo que la capacidad de aprendizaje del sistema por sí mismo es baja.</li>
<li><strong>Dominios complejos</strong>: si el corpus demasiado complejo, la creación del sistema RB puede llevar mucho tiempo y análisis. La identificación de patrones complejos es una tarea desafiante en el enfoque RB.</li>
</ul>
</div>
<div id="modelos-de-deep-learning" class="section level2">
<h2><span class="header-section-number">3.5</span> Modelos de “<em>deep-learning</em>”</h2>
<p>Cuando se busca mejorar el aprendizaje automático, generalmente se piensa en la eficiencia y la precisión, pero la dimensión más importante es la generalidad. Este es el modelo estadístico que usa <em>spaCy</em>.</p>
<p>La mayoría de los problemas de <code>NLP</code> pueden reducirse a problemas de aprendizaje automático que toman uno o más textos como entrada. Si podemos transformar estos textos en vectores, podemos reutilizar soluciones de aprendizaje profundo (<em>deep-learning</em>) de propósito general.</p>
<p>Las representaciones de palabras embebidas (<em>embebed-words</em>), también conocidas como “vectores de palabras” (<em>word-vectors</em>), son una de las tecnologías de procesamiento de lenguaje natural más utilizadas en el estado del arte actual. El modelo de <em>deep learning</em> utilizado por <em>spaCy</em> puede ser descripto en cuatro pasos.</p>
<p>Los <em>word-embeddings</em> permiten tratar a las palabras individuales como “unidades de significado”, en lugar de identificaciones completamente distintas. A este proceso se le conoce como <strong><em>embed</em></strong>.</p>
<p>Sin embargo, la mayoría de los problemas de <em>NLP</em> requieren la comprensión de tramos de texto más largos, no solo palabras individuales. Al juntar un conjunto de <em>word-embeddings</em> en una secuencia de vectores, se usan RNN bidireccionales para codificar los vectores en una matriz de oración. Las filas de esta matriz pueden entenderse como “vectores-de-tokens”: son sensibles al contexto del token dentro de la oración. Este paso se lo llama <strong><em>encode</em></strong>.</p>
<p>Finalmente, el mecanismo de <strong><em>attend</em></strong> le permite reducir la matriz de oración a un vector de oración, listo para la predicción (<strong><em>predict</em></strong>).</p>
<p>De esta manera quedan definidas las piezas que describen al modelo de <em>spaCy</em>:</p>
<div class="figure" style="text-align: center"><span id="fig:formula-shapes"></span>
<img src="assets/deep-learning-formula-nlp_shapes.png" alt="Estados posibles para las diferentes etapas de la CNN."  />
<p class="caption">
Figura 3.3: Estados posibles para las diferentes etapas de la CNN.
</p>
</div>
<p>Estos 4 procesos seran descriptos en detalle en las siguientes secciones.</p>
<div id="embed" class="section level3">
<h3><span class="header-section-number">3.5.1</span> <em>Embed</em></h3>
<blockquote>
<p>Resuelve el problema: “todas las palabras se ven iguales para la computadora”</p>
</blockquote>
<p>La idea de <em>word embeddings</em> es la de “embeber” el conjunto de tokens que componen términos con información adicional. El resultado de esta operación es una estructura abstracta que puede ser descripta como un “vector de significado”.</p>
<div class="figure" style="text-align: center"><span id="fig:formula-embed"></span>
<img src="assets/deep-learning-formula-nlp_embed.svg" alt="Embed."  />
<p class="caption">
Figura 3.4: Embed.
</p>
</div>
<p>Es importante destacar que en la etapa de <em>embed</em>, toda la información de significado es principalmente independiente del contexto en el cual esta siendo utilizada y por esta razón es facilmente obtenible del corpus no tagueado de datos (los algoritmos pueden darse cuenta de que palabras están relacionadas entre sí de manera eficiente y no supervisada).</p>
<p>Esto permite al modelo poder inferir significado a partir de la información no anotada dentro del problema en particular a resolver (el de <em>NER</em>).</p>
<p>Una tabla de <em>word-embeddings</em> mapea vectores largos binarios y esparsos en vectores cortos densos y continuos, cargados de significado relevante. Existen varias estrategias para enriquecer los <em>tokens</em> con información adicional. Las dos fuentes más grandes de información embebida son las de <strong>información lingüística</strong> y los <strong><em>word-vectors</em></strong>.</p>
<div id="información-ligüística" class="section level4">
<h4><span class="header-section-number">3.5.1.1</span> Información ligüística</h4>
<p>El objetivo de ésta etapa es la de encontrar las características intrínsecas de cada palabra. Las principales características lingüísticas detectadas están resumidas en el siguiente ejemplo.</p>
<div class="figure" style="text-align: center"><span id="fig:formula-pos"></span>
<img src="assets/pos.png" alt="Características lingüísticas."  />
<p class="caption">
Figura 3.5: Características lingüísticas.
</p>
</div>
<ul>
<li><strong>Text</strong>: la palabra original texto.</li>
<li><strong>Lemma</strong>: La forma básica de la palabra. <code>Es -&gt; ser</code>. Esto permite a las siguientes etapas trabajar con una definicón canónica del token.</li>
<li><strong>POS</strong>: la etiqueta simple de parte-del-discurso (POS).</li>
<li><strong>Tag</strong>: la etiqueta detallada de parte del discurso (POS).</li>
<li><strong>Dep</strong>: dependencia sintáctica, es decir, la relación entre tokens.</li>
<li><strong>Shape</strong>: la forma de la palabra: mayúsculas, puntuación, dígitos. Muy útil para detectar patrones como números telefónicos, documentos de identidad, CBUs, etc.</li>
<li><strong>is alpha</strong>: ¿el token es una secuencia de caracteres alfabéticos?</li>
<li><strong>is stop</strong> : ¿es el token parte de una lista de “palabras de <em>stop</em>”, es decir, las palabras más comunes del idioma?</li>
</ul>
<div id="word-vectors" class="section level5">
<h5><span class="header-section-number">3.5.1.1.1</span> <em>Word Vectors</em></h5>
<p>A diferencia de la información lingüística que es obtenida en el momento, existen otros tipos de <em>embeddings</em> más poderosos que son los productos de pre-entrenamientos; como el caso de los <em>word-vectors</em>. Los mismos se generan mediante la concatenación de atributos léxicos como <em>NORM</em>, <em>PREFIX</em>, <em>SUFFIX</em> y <em>SHAPE</em>. Luego se usa una capa oculta de una red neuronal convolucional (<em>CNN</em>) para permitir una combinación no lineal de la información en estos vectores concatenados.</p>
<p>Los <em>word-vectors</em> son particularmente útiles para términos que no están bien representados en los datos de entrenamiento etiquetados. En nuestro uso de reconocimiento de entidades, no siempre habrá suficientes ocurrencias. Por ejemplo, en los datos de entrenamiento es posible que existan ocurrencias del término “Coca-Cola”, pero ninguna del término “Manaos”.
Es interesante pensar a las palabras como “vectores de significado”. Dentro del espacio vectorial de significados el vector “perro” se encuentra cercano al de “cachorro”, “beagle”, “bulldog”, “poodle”. Esto permite al modelo poder inferir nuevas relaciones en base a una cantidad reducida de entradas.
Los <em>word-vectors</em> ponen ese hecho a disposición del modelo de reconocimiento de entidades. Si bien no existen ejemplos de “Manaos” etiquetados como “Producto”; se verá que “Manaos” tiene un vector de palabras que generalmente corresponde a los términos de un producto, por lo que puede hacer la inferencia. Y si se quiere, se puede llegar incluso al detalle de que es una “Gaseosa”.</p>
<p>Otra forma interesante de analizar y entender los <em>word-vectors</em> en su contexto de espacio vectorial multidimensional de significados es a través del algebra de vectores, como se muestra en el trabajo “<em>Towards Understanding Linear Word Analogies</em>” <span class="citation">(Ethayarajh, Duvenaud, &amp; Hirst, <a href="#ref-ethayarajh-etal-2019-towards">2019</a>)</span>:</p>
<div class="figure" style="text-align: center"><span id="fig:vec-parallelogram"></span>
<img src="assets/parallelogram.png" alt="Parallelogram structure in the vector space (by definition)." width="60%" />
<p class="caption">
Figura 3.6: Parallelogram structure in the vector space (by definition).
</p>
</div>
<p>Es facil de entender viendo la figura <a href="state-of-art.html#fig:vec-parallelogram">3.6</a> que al realizar algebra entre los diferentes vectores de significado se pueden inferir nuevos conceptos:</p>
<p><span class="math display">\[\vec{Rey} - \vec{Hombre} \approx \vec{Realeza}\]</span>
<span class="math display">\[\vec{Rey} - \vec{Hombre} + \vec{Mujer} \approx \vec{Reina}\]</span></p>
</div>
</div>
</div>
<div id="encode" class="section level3">
<h3><span class="header-section-number">3.5.2</span> <em>Encode</em></h3>
<blockquote>
<p>Resuelve el problema: el contexto de los significados es relevante y esta siendo descartado,</p>
</blockquote>
<p>El resultado de esta etapa es la de codificar vectores <strong>independientes-de-contexto</strong> en matrices de oración <strong>sensibles-al-contexto</strong>.</p>
<div class="figure" style="text-align: center"><span id="fig:formula-encode"></span>
<img src="assets/deep-learning-formula-nlp_encode.svg" alt="Encode."  />
<p class="caption">
Figura 3.7: Encode.
</p>
</div>
<p>La tecnología utilizada para esta etapa es una Red Neuronal Convolucional en la que la oración es analizada como una <em>moving window</em> de tres vectores en la que cada vector analiza su significado en relación con un vector previo y un vector posterior. Es decir, cada vector se estudia dentro del contexto de los dos vectores que lo rodean. Luego los vectores subsiguientes se evaluan de igual manera, por lo que se genera naturalmente un “efecto de decaimiento” en el que el contexto de los vectores más lejanos tiene una relevancia cada vez menor.</p>
</div>
<div id="attend" class="section level3">
<h3><span class="header-section-number">3.5.3</span> <em>Attend</em></h3>
<blockquote>
<p>Resuelve el problema: tenemos demasiada información para inferir un significado específico al problema a resolver.</p>
</blockquote>
<p>En esta etapa toda la información generada en las etapas anteriores es analizada a través de un vector de entrada o tambien conocido como “vector de consulta” o “vector de contexto” representado en la figura <a href="state-of-art.html#fig:formula-attend">3.8</a> como un vector mas oscuro.</p>
<div class="figure" style="text-align: center"><span id="fig:formula-attend"></span>
<img src="assets/deep-learning-formula-nlp_attend.svg" alt="Attend."  />
<p class="caption">
Figura 3.8: Attend.
</p>
</div>
<p>Al reducir la matriz a un vector, necesariamente se está perdiendo información. Es por eso que el “vector de contexto” es crucial: Indica que información descartar, de modo que el “vector resumen” se adapte a la red que lo consume.</p>
<p>El análisis de estas estrategias de consulta escapa el alcance de este trabajo pero resulta un tema interesante de investigación en sí mismo. Por ejemplo, investigaciones recientes han demostrado que el mecanismo de atención es una técnica flexible y que se pueden usar nuevas variaciones para crear soluciones elegantes y poderosas. Por ejemplo, en el estudio de Ankur Parikh et al <span class="citation">(Parikh, Täckström, Das, &amp; Uszkoreit, <a href="#ref-parikh-etal-2016-decomposable">2016</a>)</span> presentan un mecanismo de atención que toma dos matrices de oraciones y genera un solo vector.</p>
</div>
<div id="predict" class="section level3">
<h3><span class="header-section-number">3.5.4</span> <em>Predict</em></h3>
<blockquote>
<p>Resuelve el problema: necesito un valor específico y no una representación genérica abstracta.</p>
</blockquote>
<p>Finalmente en esta etapa tenemos un nuevo “vector de significado” que resulta de la consulta a la etapa anterior. Es necesario ahora traducir este vector a un <em>token</em> efectivo. En el caso de <em>NER</em>, el <em>token</em> que interesa obtener es el de la etiqueta de entidad.</p>
<div class="figure" style="text-align: center"><span id="fig:formula-predict"></span>
<img src="assets/deep-learning-formula-nlp_predict.svg" alt="Predict."  />
<p class="caption">
Figura 3.9: Predict.
</p>
</div>
</div>
</div>
<div id="use-of-cnn" class="section level2">
<h2><span class="header-section-number">3.6</span> Uso del modelo estadístico</h2>
<p>Experimentos en inglés, holandés, alemán y español muestran que se pueden obtener resultados a-la-par del estado del arte mediante el uso de un “autómata finito determinístico de pila” en conjunto con una red neuronal <span class="citation">(Lample, Ballesteros, Subramanian, Kawakami, &amp; Dyer, <a href="#ref-DBLP:journals/corr/LampleBSKD16">2016</a>)</span>.</p>
<p>Este autómata de pila es el nexo entre la Red Neuronal Convolucional (<em>CNN</em>) que contiene el modelo estadístico para predecir entidades en el texto completo. En el mismo, se envían cada uno de los estados porl los que se mueve para ir generando entidades con una herística del tipo <em>greedy</em>.</p>
<p>La figura <a href="state-of-art.html#fig:lampe-1">3.10</a> muestra las posibles acciones de transición de este autómata.</p>
<ul>
<li>SHIFT: consume una token del input y al mueve al stack para generar una nueva entidad.</li>
<li>REDUCE: mueve el stack actual al output tagueado como entity.</li>
<li>OUT: consume una token del input y la mueve sl output directamente.</li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:lampe-1"></span>
<img src="assets/lampe_1.png" alt="Transiciones del modelo Stack-LSTM indicando la acción aplicada y el estado resultante."  />
<p class="caption">
Figura 3.10: Transiciones del modelo Stack-LSTM indicando la acción aplicada y el estado resultante.
</p>
</div>
<p>Para saber que acción tomar se consulta el modelo estadístico. En la figura <a href="state-of-art.html#fig:lampe-2">3.11</a> se puede ver un ejemplo de como se recorre una oración bajo el stack propuesto:</p>
<div class="figure" style="text-align: center"><span id="fig:lampe-2"></span>
<img src="assets/lampe_2.png" alt="Secuencia de tranciciones para el ejemplo &quot;Mark Watney visited Mars&quot; en el modelo de Stack-LSTM."  />
<p class="caption">
Figura 3.11: Secuencia de tranciciones para el ejemplo “Mark Watney visited Mars” en el modelo de Stack-LSTM.
</p>
</div>
<ul>
<li>Primero se empieza con un stack vacío.</li>
<li>Se consume <em>“Mark”</em> y la <em>CNN</em> predice que es una posible Persona. Lo envia al stack.</li>
<li>Se consume <em>“Watney”</em> y la <em>CNN</em> predice que es una posible continuación de Persona. Lo envia al stack.</li>
<li>Se consume <em>“visited”</em> y la <em>CNN</em> predice que esto no forma parte de una entidad. Por lo tanto antes se <em>REDUCE</em> la entidad <em>“Mark Watney”</em> del stack actual.</li>
<li>Análogamente se detecta la entidad <em>“Mars”</em></li>
</ul>
<p>La predicción que realiza la <em>CNN</em> tiene un puntaje de 0 a 1; un porcentaje de certeza de que la token sea de un tipo específico. A veces las tokens pueden tener más de una etiqueta candidata a ser utilizada y éste es el método para desempatar este conflicto. Cuanto más cercano a <span class="math inline">\(0.5\)</span> es ese punaje mayor es la incerteza.</p>

<div style="page-break-after: always;"></div>
</div>
</div>
<h3>Referencias</h3>
<div id="refs" class="references">
<div id="ref-github_machine_learning">
<p>Elliott, T. (2019). The state of the octoverse: Machine learning. Retrieved January 24, 2019, from <a href="https://github.blog/2019-01-24-the-state-of-the-octoverse-machine-learning/">https://github.blog/2019-01-24-the-state-of-the-octoverse-machine-learning/</a></p>
</div>
<div id="ref-ethayarajh-etal-2019-towards">
<p>Ethayarajh, K., Duvenaud, D., &amp; Hirst, G. (2019). Towards understanding linear word analogies. <em>Proceedings of the 57th annual meeting of the association for computational linguistics</em>, 3253–3262. <a href="https://doi.org/10.18653/v1/P19-1315">https://doi.org/10.18653/v1/P19-1315</a></p>
</div>
<div id="ref-DBLP:journals/corr/LampleBSKD16">
<p>Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K., &amp; Dyer, C. (2016). Neural architectures for named entity recognition. <em>CoRR</em>, <em>abs/1603.01360</em>. Retrieved from <a href="http://arxiv.org/abs/1603.01360">http://arxiv.org/abs/1603.01360</a></p>
</div>
<div id="ref-parikh-etal-2016-decomposable">
<p>Parikh, A., Täckström, O., Das, D., &amp; Uszkoreit, J. (2016). A decomposable attention model for natural language inference. <em>Proceedings of the 2016 conference on empirical methods in natural language processing</em>, 2249–2255. <a href="https://doi.org/10.18653/v1/D16-1244">https://doi.org/10.18653/v1/D16-1244</a></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="problem-definition.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="implementation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/qcho/nerd-pf/edit/master/03-state-of-art.Rmd",
"text": "Editar"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["nerd-pf.pdf", "nerd-pf.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
